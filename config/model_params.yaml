# File: config/model_params.yaml

strategies:
  full_comparison:
    # --- BENCHMARKS ESTATÍSTICOS (Mantidos iguais, pois já são fortes) ---
    - model_name: "base_arima"
      model_type: "ARIMA"
      comparison_group: "benchmark_statistical"
      arima_params:
        start_p: 1
        start_q: 1
        max_p: 5
        max_q: 5
        d: null
        D: null
        trace: false

    - model_name: "standalone_ets"
      model_type: "ETS"
      comparison_group: "benchmark_statistical"
      ets_params:
        trend: "add"
        seasonal: "add"
        damped_trend: true
        trace: false

    - model_name: "standalone_naive"
      model_type: "NAIVE"
      comparison_group: "benchmark_statistical"

    - model_name: "standalone_seasonal_naive"
      model_type: "SEASONAL_NAIVE"
      comparison_group: "benchmark_statistical"

    # --- MODELOS DEEP LEARNING (KERAS) - CONFIGURAÇÃO MAIS ROBUSTA ---
    - model_name: "standalone_mlp_direct"
      model_type: "MLP_Direct"
      comparison_group: "benchmark_standalone_dl"
      mlp_params:
        epochs: 100
        batch_size: 32
        patience: 10
        hpo_space:
          input_lags: ["int", 12, 36] # Busca entre 1 e 3 anos de histórico
          learning_rate: ["float", 1e-4, 1e-2, "log"]
          n_layers: ["int", 2, 4] # Redes mais profundas
          n_neurons_l1: ["int", 64, 256]
          n_neurons_l2: ["int", 32, 128]
          n_neurons_l3: ["int", 16, 64]

    - model_name: "standalone_mlp_mimo"
      model_type: "MLP_MIMO"
      comparison_group: "benchmark_standalone_dl"
      mlp_params:
        epochs: 100
        batch_size: 32
        patience: 20
        hpo_space:
          input_lags: ["int", 12, 36]
          learning_rate: ["float", 1e-4, 1e-2, "log"]
          n_layers: ["int", 2, 4]
          n_neurons_l1: ["int", 64, 256]
          n_neurons_l2: ["int", 32, 128]
          n_neurons_l3: ["int", 16, 64]

    - model_name: "standalone_lstm"
      model_type: "LSTM"
      comparison_group: "benchmark_standalone_dl"
      lstm_params:
        epochs: 100
        batch_size: 32
        patience: 10
        hpo_space:
          input_lags: ["int", 12, 36]
          learning_rate: ["float", 1e-4, 1e-2, "log"]
          n_layers: ["int", 1, 2]
          n_neurons_l1: ["int", 50, 150] # LSTMs precisam de menos neurônios que MLPs
          n_neurons_l2: ["int", 25, 75]

    # --- MODELOS NEURALFORECAST (N-BEATS, N-HiTS, Transformer) ---

    - model_name: "standalone_itransformer"
      model_type: "iTransformer"
      comparison_group: "benchmark_standalone_dl"
      itransformer_params:
        model_kwargs:
          input_size: 24
          n_heads: 4
          hidden_size: 128
          n_series: 1
          random_seed: 42
        trainer_kwargs:
          max_steps: 100
          early_stop_patience_steps: 10

    - model_name: "standalone_nhits"
      model_type: "NHiTS"
      comparison_group: "benchmark_standalone_dl"
      nhits_params:
        model_kwargs:
          input_size: 24
          stack_types: ['identity', 'identity']
          n_blocks: [3, 3]
          mlp_units: [[128, 128], [128, 128]]
          n_pool_kernel_size: [2, 1]
          random_seed: 42
        trainer_kwargs:
          max_steps: 100
          early_stop_patience_steps: 10

    - model_name: "standalone_nbeats_mimo"
      model_type: "NBEATS_MIMO"
      comparison_group: "benchmark_standalone_dl"
      nbeats_params:
        model_kwargs:
          input_size: 24
          stack_types: ["trend", "seasonality"]
          n_blocks: [3, 3]
          n_polynomials: 2
          n_harmonics: 1
          random_seed: 42
        trainer_kwargs:
          max_steps: 100
          early_stop_patience_steps: 10

    - model_name: "standalone_nbeats_direct"
      model_type: "NBEATS_Direct"
      comparison_group: "benchmark_standalone_dl"
      nbeats_params:
        model_kwargs:
          input_size: 24
          stack_types: ["identity", "identity"]
          n_blocks: [3, 3]
          n_polynomials: 2
          n_harmonics: 1
          random_seed: 42
        trainer_kwargs:
          max_steps: 100
          early_stop_patience_steps: 10

    # --- MODELOS HÍBRIDOS (ARIMA + DL) ---

    - model_name: "hybrid_arima_mlp_direct"
      model_type: "Hybrid_MLP_Direct"
      comparison_group: "hybrid_direct"
      depends_on: "base_arima"
      mlp_params:
        epochs: 100
        batch_size: 16
        patience: 10
        hpo_space:
          input_lags: ["int", 12, 24] # Resíduos podem ter memória mais curta, mas 12 é bom
          learning_rate: ["float", 1e-4, 1e-2, "log"]
          n_layers: ["int", 2, 3]
          n_neurons_l1: ["int", 32, 128]
          n_neurons_l2: ["int", 16, 64]
          n_neurons_l3: ["int", 8, 32]

    - model_name: "hybrid_arima_mlp_mimo"
      model_type: "Hybrid_MLP_MIMO"
      comparison_group: "hybrid_mimo"
      depends_on: "base_arima"
      mlp_params:
        epochs: 100
        batch_size: 16
        patience: 10
        hpo_space:
          input_lags: ["int", 12, 24]
          learning_rate: ["float", 1e-4, 1e-2, "log"]
          n_layers: ["int", 2, 3]
          n_neurons_l1: ["int", 32, 128]
          n_neurons_l2: ["int", 16, 64]
          n_neurons_l3: ["int", 8, 32]

    - model_name: "hybrid_arima_mlp_recursive"
      model_type: "Hybrid_MLP_Recursive"
      comparison_group: "hybrid_recursive"
      depends_on: "base_arima"
      mlp_params:
        epochs: 100
        batch_size: 16
        patience: 10
        hpo_space:
          input_lags: ["int", 12, 24]
          learning_rate: ["float", 1e-4, 1e-2, "log"]
          n_layers: ["int", 2, 3]
          n_neurons_l1: ["int", 32, 128]
          n_neurons_l2: ["int", 16, 64]
          n_neurons_l3: ["int", 8, 32]

    - model_name: "hybrid_arima_lstm_recursive"
      model_type: "Hybrid_LSTM_Recursive"
      comparison_group: "hybrid_recursive"
      depends_on: "base_arima"
      lstm_params:
        epochs: 100
        batch_size: 16
        patience: 10
        hpo_space:
          input_lags: ["int", 12, 24]
          learning_rate: ["float", 1e-4, 1e-2, "log"]
          n_layers: ["int", 1, 2]
          n_neurons_l1: ["int", 32, 100]
          n_neurons_l2: ["int", 16, 50]

    - model_name: "hybrid_arima_nhits_mimo"
      model_type: "Hybrid_MIMO_NHITS"
      comparison_group: "hybrid_mimo"
      depends_on: "base_arima"
      nhits_params:
        model_kwargs:
          input_size: 24
          stack_types: ['identity', 'identity']
          n_blocks: [3, 3]
          mlp_units: [[128, 128], [128, 128]]
          random_seed: 42
        trainer_kwargs:
          max_steps: 100
          early_stop_patience_steps: 10

    - model_name: "hybrid_arima_nhits_direct"
      model_type: "Hybrid_Direct_NHITS"
      comparison_group: "hybrid_direct"
      depends_on: "base_arima"
      nhits_params:
        model_kwargs:
          input_size: 24
          stack_types: ['identity', 'identity']
          n_blocks: [3, 3]
          mlp_units: [[128, 128], [128, 128]]
          random_seed: 42
        trainer_kwargs:
          max_steps: 100
          early_stop_patience_steps: 10

    - model_name: "hybrid_arima_nbeats_mimo_nf"
      model_type: "Hybrid_MIMO_NBEATS_NF"
      comparison_group: "hybrid_mimo"
      depends_on: "base_arima"
      nbeats_params:
        model_kwargs:
          input_size: 24
          stack_types: ["identity", "identity"]
          n_blocks: [2, 2]
          n_polynomials: 2
          n_harmonics: 1
          random_seed: 42
        trainer_kwargs:
          max_steps: 100
          early_stop_patience_steps: 10

    - model_name: "hybrid_arima_nbeats_direct_nf"
      model_type: "Hybrid_Direct_NBEATS_NF"
      comparison_group: "hybrid_direct"
      depends_on: "base_arima"
      nbeats_params:
        model_kwargs:
          input_size: 24
          stack_types: ["identity", "identity"]
          n_blocks: [2, 2]
          n_polynomials: 2
          n_harmonics: 1
          random_seed: 42
        trainer_kwargs:
          max_steps: 100
          early_stop_patience_steps: 10
